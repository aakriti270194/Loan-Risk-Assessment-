{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b8c54c",
   "metadata": {},
   "source": [
    "# Capstone Analysis Notebook\n",
    "\n",
    "This notebook contains the step-by-step pipeline for your capstone:\n",
    "\n",
    "- Phase 1: Descriptive analytics (EDA)\n",
    "- Phase 2: Unsupervised ML (clustering)\n",
    "- Phase 2: Feature selection\n",
    "- Phase 3: Supervised ML comparison\n",
    "\n",
    "**Dataset path:** `/mnt/data/Data_08_Simulated Loan Risk Assessment Data.csv`\n",
    "\n",
    "Run cells sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc0cf217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready. Dataset path: /mnt/data/Data_08_Simulated Loan Risk Assessment Data.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,4)\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "DATA_PATH = r\"/mnt/data/Data_08_Simulated Loan Risk Assessment Data.csv\"\n",
    "SUMMARY_OUT = r\"/mnt/data/capstone_analysis_summary.csv\"\n",
    "\n",
    "print('Ready. Dataset path:', DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "904481fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data_08_Simulated Loan Risk Assessment Data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Cell 2: Load dataset & preview\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData_08_Simulated Loan Risk Assessment Data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape:\u001b[39m\u001b[38;5;124m'\u001b[39m, df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      4\u001b[0m display(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data_08_Simulated Loan Risk Assessment Data.csv'"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load dataset & preview\n",
    "df = pd.read_csv ('Data_08_Simulated Loan Risk Assessment Data.csv')\n",
    "print('Shape:', df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e064d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: dtypes & missing values\n",
    "display(df.dtypes)\n",
    "display(df.isnull().sum().sort_values(ascending=False).head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f28f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Descriptive statistics & central tendency\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "display(df[num_cols].describe().T)\n",
    "\n",
    "ct = df[num_cols].agg(['mean','median','std','min','max']).T\n",
    "ct['mode'] = df[num_cols].mode().iloc[0]\n",
    "ct['skew'] = df[num_cols].skew()\n",
    "ct['kurtosis'] = df[num_cols].kurtosis()\n",
    "display(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d450a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Histograms and boxplots for numeric columns\n",
    "for col in num_cols:\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12,3))\n",
    "    sns.histplot(df[col].dropna(), kde=True, ax=axes[0])\n",
    "    axes[0].set_title(f'Histogram: {col}')\n",
    "    sns.boxplot(x=df[col], ax=axes[1])\n",
    "    axes[1].set_title(f'Boxplot: {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eded01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Candidate target detection\n",
    "candidate_targets = [c for c in df.columns if df[c].nunique() <= 10]\n",
    "print('Candidate targets (<=10 unique):', candidate_targets)\n",
    "target = 'loan_status' if 'loan_status' in df.columns else (candidate_targets[0] if candidate_targets else None)\n",
    "print('Using target column:', target)\n",
    "if target:\n",
    "    display(df[target].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a32d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Impute and scale numeric features\n",
    "imp = SimpleImputer(strategy='median')\n",
    "X_num = pd.DataFrame(imp.fit_transform(df[num_cols]), columns=num_cols)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_num), columns=num_cols)\n",
    "print('Prepared numeric feature matrix:', X_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: PCA(2) for visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print('Explained variance ratio (PC1,PC2):', pca.explained_variance_ratio_)\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], s=10)\n",
    "plt.title('PCA(2) projection of numeric features')\n",
    "plt.xlabel('PC1'); plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbfb7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: KMeans silhouette search (k=2..6)\n",
    "best_k, best_score = None, -1\n",
    "for k in range(2,7):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labs = kmeans.fit_predict(X_scaled)\n",
    "    try:\n",
    "        s = silhouette_score(X_scaled, labs)\n",
    "    except:\n",
    "        s = -1\n",
    "    print(f'k={k} silhouette={s:.4f}')\n",
    "    if s > best_score:\n",
    "        best_score = s; best_k = k; best_labels = labs; best_kmeans = kmeans\n",
    "\n",
    "print('Best k:', best_k, 'score:', best_score)\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=best_labels, s=10)\n",
    "plt.title(f'KMeans clusters (k={best_k}) projected to PCA(2)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c0636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Compare clusters to actual target\n",
    "if target and target in df.columns:\n",
    "    print('Crosstab of KMeans clusters vs target')\n",
    "    display(pd.crosstab(best_labels, df[target].fillna('MISSING')))\n",
    "    try:\n",
    "        ari = adjusted_rand_score(pd.factorize(df[target].fillna(df[target].mode()[0]))[0], best_labels)\n",
    "        print('Adjusted Rand Index:', ari)\n",
    "    except Exception as e:\n",
    "        print('Could not compute ARI:', e)\n",
    "else:\n",
    "    print('No suitable target column found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Correlation matrix & high-correlation pairs\n",
    "corr = X_num.corr().abs()\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(corr, cmap='viridis', vmax=1, vmin=0)\n",
    "plt.title('Absolute correlation matrix (numeric features)')\n",
    "plt.show()\n",
    "\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "high_corr = [(i,j,upper.loc[i,j]) for i in upper.index for j in upper.columns if (not pd.isna(upper.loc[i,j])) and upper.loc[i,j] > 0.85]\n",
    "print('Highly correlated pairs (corr>0.85):', high_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7dba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: RandomForest feature importances and SelectFromModel\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "if 'target' not in globals():\n",
    "    target = 'loan_status' if 'loan_status' in df.columns else (candidate_targets[0] if candidate_targets else None)\n",
    "if target and target in df.columns:\n",
    "    y = pd.factorize(df[target].fillna(df[target].mode()[0]))[0]\n",
    "else:\n",
    "    y = np.zeros(X_num.shape[0], dtype=int)\n",
    "\n",
    "rf.fit(X_num, y)\n",
    "importances = pd.Series(rf.feature_importances_, index=X_num.columns).sort_values(ascending=False)\n",
    "display(importances.head(20))\n",
    "\n",
    "sel = SelectFromModel(rf, threshold='median', prefit=True)\n",
    "selected_features = X_num.columns[sel.get_support()].tolist()\n",
    "print('RF-selected features (threshold=median):', selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d308f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Optional LassoCV (if target suitable)\n",
    "try:\n",
    "    if target and target in df.columns and df[target].nunique() <= 10:\n",
    "        lasso = LassoCV(cv=5, random_state=42, max_iter=5000).fit(X_scaled, y)\n",
    "        coef = pd.Series(lasso.coef_, index=X_scaled.columns).sort_values(key=lambda x: np.abs(x), ascending=False)\n",
    "        display(coef.head(20))\n",
    "    else:\n",
    "        print('Skipping LassoCV: no suitable target or too many unique classes.')\n",
    "except Exception as e:\n",
    "    print('LassoCV error:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8a2767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Prepare classifiers and parameter grids\n",
    "features = selected_features if len(selected_features)>0 else X_num.columns.tolist()\n",
    "X = X_num[features]\n",
    "print('Feature matrix for modeling shape:', X.shape)\n",
    "\n",
    "classifiers = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=2000),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'SVC': SVC(probability=True, random_state=42),\n",
    "    'KNeighbors': KNeighborsClassifier()\n",
    "}\n",
    "param_grids = {\n",
    "    'LogisticRegression': {'C':[0.01,0.1,1]},\n",
    "    'RandomForest': {'n_estimators':[100,200], 'max_depth':[5,10,None]},\n",
    "    'GradientBoosting': {'n_estimators':[100,200], 'learning_rate':[0.01,0.1]},\n",
    "    'SVC': {'C':[0.1,1], 'kernel':['rbf','linear']},\n",
    "    'KNeighbors': {'n_neighbors':[3,5]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616aec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: GridSearchCV and evaluation across sample fractions\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "sample_fracs = [0.1, 0.3, 0.5, 1.0]\n",
    "\n",
    "for frac in sample_fracs:\n",
    "    print('\\nTraining fraction:', frac)\n",
    "    if frac < 1.0:\n",
    "        X_sub, _, y_sub, _ = train_test_split(X, y, train_size=frac, stratify=y, random_state=42)\n",
    "    else:\n",
    "        X_sub, y_sub = X, y\n",
    "\n",
    "    for name, clf in classifiers.items():\n",
    "        try:\n",
    "            grid = GridSearchCV(clf, param_grids[name], cv=skf, scoring='accuracy', n_jobs=-1)\n",
    "            grid.fit(X_sub, y_sub)\n",
    "            best = grid.best_estimator_\n",
    "            cv_score = cross_val_score(best, X_sub, y_sub, cv=skf, scoring='accuracy', n_jobs=-1).mean()\n",
    "            Xtr, Xte, ytr, yte = train_test_split(X_sub, y_sub, test_size=0.2, stratify=y_sub, random_state=42)\n",
    "            best.fit(Xtr, ytr)\n",
    "            ypred = best.predict(Xte)\n",
    "            proba = best.predict_proba(Xte)[:,1] if hasattr(best, 'predict_proba') and len(np.unique(y))==2 else None\n",
    "            results.append({\n",
    "                'frac': frac, 'classifier': name, 'best_params': grid.best_params_, 'cv_acc_mean': cv_score,\n",
    "                'test_acc': accuracy_score(yte, ypred),\n",
    "                'precision': precision_score(yte, ypred, average='binary' if len(np.unique(y))==2 else 'macro', zero_division=0),\n",
    "                'recall': recall_score(yte, ypred, average='binary' if len(np.unique(y))==2 else 'macro', zero_division=0),\n",
    "                'f1': f1_score(yte, ypred, average='binary' if len(np.unique(y))==2 else 'macro', zero_division=0),\n",
    "                'roc_auc': roc_auc_score(yte, proba) if proba is not None else np.nan\n",
    "            })\n",
    "            print(f'{name} done. cv_acc={cv_score:.3f}')\n",
    "        except Exception as e:\n",
    "            print(f'Error with {name}:', e)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(['frac','cv_acc_mean'], ascending=[True, False])\n",
    "display(results_df.head(50))\n",
    "results_df.to_csv(SUMMARY_OUT, index=False)\n",
    "print('Saved summary to:', SUMMARY_OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254aae0b",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Encode categorical features and re-run the pipeline\n",
    "- Try LightGBM / XGBoost and larger hyperparameter searches\n",
    "- Add SHAP explainability for the best model\n",
    "- Tune thresholds and calibrate probabilities for production use\n",
    "\n",
    "You can run the notebook top-to-bottom. If you want, I can also run any of these follow-ups for you now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
