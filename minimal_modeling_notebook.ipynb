{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7337ef",
   "metadata": {},
   "source": [
    "# Minimal Modeling Notebook â€” Loan Risk (GitHub-ready)\n",
    "\n",
    "This notebook is a concise, well-documented modeling notebook suitable for publishing on GitHub. It focuses on the modeling pipeline: data load, simple preprocessing, training two models, evaluation, and saving artifacts.\n",
    "\n",
    "**Dataset path:** `/mnt/data/Data_08_Simulated Loan Risk Assessment Data.csv`\n",
    "\n",
    "Run cells sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a554529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and config\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8,4)\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "DATA_PATH = r\"/mnt/data/Data_08_Simulated Loan Risk Assessment Data.csv\"\n",
    "print('Dataset path:', DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ae7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load data and quick inspect\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Shape:', df.shape)\n",
    "display(df.head())\n",
    "print('\\nData types:')\n",
    "display(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322bb277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Choose target and brief check\n",
    "# Use 'loan_status' if present otherwise pick a low-cardinality column\n",
    "target = 'loan_status' if 'loan_status' in df.columns else ([c for c in df.columns if df[c].nunique()<=10][0] if any(df[c].nunique()<=10 for c in df.columns) else None)\n",
    "print('Target chosen:', target)\n",
    "if target is None:\n",
    "    raise ValueError('No suitable target found. Set target manually.')\n",
    "display(df[target].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f0c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Split features and target, identify column types\n",
    "X = df.drop(columns=[target]).copy()\n",
    "y = pd.factorize(df[target].fillna(df[target].mode()[0]))[0]\n",
    "\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "print('Numeric cols:', num_cols)\n",
    "print('Categorical cols:', cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f77171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Preprocessing pipelines (simple & reproducible)\n",
    "num_pipe = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))])\n",
    "preprocessor = ColumnTransformer([('num', num_pipe, num_cols), ('cat', cat_pipe, cat_cols)])\n",
    "\n",
    "# Quick transform to show resultant shape\n",
    "X_trans = preprocessor.fit_transform(X)\n",
    "print('Transformed feature matrix shape:', X_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd513f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Train-test split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "print('Train size:', X_tr.shape[0], 'Test size:', X_te.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Train Logistic Regression (pipeline)\n",
    "pipe_lr = Pipeline([('pre', preprocessor), ('clf', LogisticRegression(max_iter=1000))])\n",
    "pipe_lr.fit(X_tr, y_tr)\n",
    "pred_lr = pipe_lr.predict(X_te)\n",
    "proba_lr = pipe_lr.predict_proba(X_te)[:,1] if hasattr(pipe_lr.named_steps['clf'], 'predict_proba') and len(np.unique(y))==2 else None\n",
    "\n",
    "print('Logistic Regression metrics:')\n",
    "print('Accuracy:', accuracy_score(y_te, pred_lr))\n",
    "print('F1:', f1_score(y_te, pred_lr, average='binary' if len(np.unique(y))==2 else 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c13ce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Train RandomForest (pipeline)\n",
    "pipe_rf = Pipeline([('pre', preprocessor), ('clf', RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1))])\n",
    "pipe_rf.fit(X_tr, y_tr)\n",
    "pred_rf = pipe_rf.predict(X_te)\n",
    "proba_rf = pipe_rf.predict_proba(X_te)[:,1] if hasattr(pipe_rf.named_steps['clf'], 'predict_proba') and len(np.unique(y))==2 else None\n",
    "\n",
    "print('RandomForest metrics:')\n",
    "print('Accuracy:', accuracy_score(y_te, pred_rf))\n",
    "print('F1:', f1_score(y_te, pred_rf, average='binary' if len(np.unique(y))==2 else 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a7c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Compare models (detailed metrics and confusion matrices)\n",
    "def eval_model(y_true, y_pred, y_proba=None):\n",
    "    res = {'accuracy': accuracy_score(y_true,y_pred),\n",
    "           'precision': precision_score(y_true,y_pred, average='binary' if len(np.unique(y))==2 else 'macro', zero_division=0),\n",
    "           'recall': recall_score(y_true,y_pred, average='binary' if len(np.unique(y))==2 else 'macro', zero_division=0),\n",
    "           'f1': f1_score(y_true,y_pred, average='binary' if len(np.unique(y))==2 else 'macro', zero_division=0),\n",
    "           'roc_auc': roc_auc_score(y_true,y_proba) if y_proba is not None else np.nan}\n",
    "    return res\n",
    "\n",
    "res_lr = eval_model(y_te, pred_lr, proba_lr)\n",
    "res_rf = eval_model(y_te, pred_rf, proba_rf)\n",
    "results_df = pd.DataFrame([res_lr, res_rf], index=['LogisticRegression','RandomForest'])\n",
    "display(results_df)\n",
    "\n",
    "print('\\nConfusion matrix (RandomForest):')\n",
    "print(confusion_matrix(y_te, pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10cf244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Save best model and results\n",
    "# Choose RandomForest as an example \"best\" model (you can change this logic)\n",
    "joblib.dump(pipe_rf, '/mnt/data/best_model.pkl')\n",
    "results_df.to_csv('/mnt/data/model_results_summary.csv', index=True)\n",
    "print('Saved model to /mnt/data/best_model.pkl')\n",
    "print('Saved results to /mnt/data/model_results_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247a97b7",
   "metadata": {},
   "source": [
    "## How to use this notebook on GitHub\n",
    "\n",
    "- Include `requirements.txt` listing the libraries (scikit-learn, pandas, numpy, matplotlib, seaborn, joblib).\n",
    "- Add a short `README.md` explaining how to run the notebook and what each cell does.\n",
    "- Use Git LFS for storing any large model artifacts (if needed)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
